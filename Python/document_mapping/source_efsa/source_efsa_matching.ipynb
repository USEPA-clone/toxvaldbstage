{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae5712f0",
   "metadata": {},
   "source": [
    "Cells below are used in the matching of records in source_efsa_raw to clowder information in the index files.\n",
    "\n",
    "Major goals are to...\n",
    "\n",
    "    Initialize the kernel by importing modules, loading the environment file, and the necessary excel sheets as dataframes\n",
    "    \n",
    "    Create a \"key\" that holds only unique entries in source_efsa_raw\n",
    "    \n",
    "    Match entries in the key to the index files and fill in clowder information (id and file name) based on the record_url and long_ref fields\n",
    "    \n",
    "    Matching key entries to source_efsa_raw entries and copy over the clowder information\n",
    "    \n",
    "    Output a spreadsheets of key checkpoints for inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7319c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script to match index entries to EFSA data\n",
    "\n",
    "import pandas as pd #For dealing with excel spreadsheets as dataframes\n",
    "import numpy as np #Basic math and functions. Could be useful\n",
    "import requests #For API interactions\n",
    "import time as t #sleep times\n",
    "from dotenv import load_dotenv #environment files for credentials. Make sure to pip install dotenv first\n",
    "import os #For accessing environment file\n",
    "\n",
    "#environment file with credentials\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc1d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the reference and raw excel sheets as pandas dataframes.\n",
    "\n",
    "raw = pd.read_excel(r\"L:\\PRIV\\ToxValDB\\Document Mapping\\source_efsa\\source_efsa_raw.xlsx\")\n",
    "master_ref_info = pd.read_excel(r\"L:\\PRIV\\ToxValDB\\Document Mapping\\source_efsa\\index files\\CCTE_Deliverable_RefinfoSheet_Master.xlsx\")\n",
    "master_track = pd.read_excel(r\"L:\\PRIV\\ToxValDB\\Document Mapping\\source_efsa\\index files\\CCTE_PDF_MasterTrackingSheet_Deliverable.xlsx\", sheet_name = 1)\n",
    "ref_info = pd.read_excel(r\"L:\\PRIV\\ToxValDB\\Document Mapping\\source_efsa\\index files\\CCTE_TSCAPOCDeliverable_RefinfoSheet.xlsx\")\n",
    "toxval = pd.read_excel(r\"L:\\PRIV\\ToxValDB\\Document Mapping\\source_efsa\\index files\\ToxValDB TSCA POC references for ICF_MasterTracking_Deliverable.xlsx\", sheet_name = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06031181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manipulate the column headers a bit. This is to ensure the headers have the same name and convention (url vs URL and Source Hash vs source_hash)\n",
    "master_ref_info.columns = master_ref_info.columns.str.lower().str.replace(' ','_')\n",
    "master_track.columns = master_track.columns.str.lower().str.replace(' ','_')\n",
    "ref_info.columns = ref_info.columns.str.lower().str.replace(' ','_')\n",
    "toxval.columns = toxval.columns.str.lower().str.replace(' ','_')\n",
    "\n",
    "#Dictionary to hold all of the dataframes\n",
    "index_files = {'master_ref_info':master_ref_info, 'master_track':master_track,'ref_info':ref_info, 'toxval':toxval}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428acfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Efsa raw is REALLY big with lots of duplicates. Lets make a \"key\" dataframe that holds only the unique entries in efsa raw\n",
    "\n",
    "#array to hold the unique urls in efsa raw\n",
    "unique_url = []\n",
    "\n",
    "#emtpy dictionary to initialize the key dataframe\n",
    "d = {}\n",
    "#inital empty dataframe\n",
    "key = pd.DataFrame(data = d)\n",
    "j = 0\n",
    "\n",
    "#look through all the urls in raw. save them to the unique_url array (used for checking later) and put them in the key \n",
    "for i in range(len(raw)):\n",
    "    url = raw.loc[i,'record_url']\n",
    "    #long_ref = raw.loc[i,'long_ref']\n",
    "    #year = str(raw.loc[i,'year'])\n",
    "    if url not in unique_url:\n",
    "        unique_url.append(url)\n",
    "#copy the data in efsa raw for each unique url into identical columns in the key dataframe\n",
    "        for column in raw.columns:\n",
    "            key.loc[j,column] = raw.loc[i,column]\n",
    "        j += 1\n",
    "\n",
    "#sanity checks\n",
    "#print(len(unique_url))\n",
    "#print(len(key))\n",
    "#key.loc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb23f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search through the index files for each unique entry in the key starting with the URLs.\n",
    "#Some manual checking, inspection, and comparison was done as the key was being made.\n",
    "#We will drop some of these columns in the final version of the key.\n",
    "\n",
    "\n",
    "#Keep track of the index file we found each entry in\n",
    "blank = ['' for x in range(len(key))]\n",
    "key['found_in'] = blank\n",
    "\n",
    "for i in range(len(key)):\n",
    "#Progress check to make sure we are moving along\n",
    "    if i%100 == 0:\n",
    "        print(i)\n",
    "#First, search for direct url matches.\n",
    "    url = key.loc[i,'record_url']\n",
    "    raw_year = int(key.loc[i,'year'])\n",
    "    for name in index_files:\n",
    "#Look through each index file for a match in the 'url' field\n",
    "        source = index_files[name]\n",
    "        source = source.fillna('')\n",
    "        other = source[source['url'].str.match(url)]\n",
    "#If there is a match, we have found the index file to reference! We can stop the loop.\n",
    "        if other.empty == False:\n",
    "            key.loc[i,'found'] = 'found'\n",
    "            break\n",
    "#If you cannot find a match in any of the index files, move on to the next url and fill that in the key\n",
    "    if other.empty == True:\n",
    "        #missing1.append(url)\n",
    "        key.loc[i,'found'] = 'not found'\n",
    "        key.loc[i,'index_long_ref'] = 'not found'\n",
    "        continue\n",
    "#I noticed some urls have several matches in an index file. The index files include duplicates.\n",
    "    idx = 0\n",
    "#If there are several matches, look at each one and see if the index file lists it as a duplicate.\n",
    "    if len(other) > 1:\n",
    "        for idx in range(len(other.index)):\n",
    "            status = other.loc[other.index[idx],'duplicate']\n",
    "#We will also keep track of the year listed in the index file. Some of these are inconsistently populated.\n",
    "            index_year = int(other.loc[other.index[idx],'year'])\n",
    "#Once we have found the primary reference, we can break out and work with it\n",
    "            if status.lower() == 'primary':\n",
    "                break\n",
    "#If the index file only has duplicates, it has the hash for the primary. Look there.\n",
    "    try:\n",
    "        status = other.loc[other.index[idx],'duplicate']\n",
    "        if status.lower() == 'duplicate':\n",
    "#Extract the record hashes listed in the excel sheets. If the match is as duplicate: Look in the primary section\n",
    "            record_hash = other.loc[other.index[idx],'primary_reference_for_duplicate_set']\n",
    "        else:\n",
    "#If the match is a primary, look at its record hash\n",
    "            record_hash = other.loc[other.index[idx],'record_source_hash']\n",
    "    except:\n",
    "#Not all index files have the duplicate field. I can only assume they only have primaries.\n",
    "        record_hash = other.loc[other.index[idx],'record_source_hash']\n",
    "#Start filling in comparison information like the long refs, full citations, etc. from both raw and the index file\n",
    "    key.loc[i,'index_long_ref'] = other.loc[other.index[idx],'long_ref']\n",
    "    key.loc[i,'index_citation'] = other.loc[other.index[idx],'citation']\n",
    "    key.loc[i,'found_in'] = key.loc[i,'found_in'] + ' ' + name\n",
    "    index_year = int(other.loc[other.index[idx],'year'])\n",
    "    key.loc[i,'index_year'] = index_year          \n",
    "    key.loc[i,'record_hash'] = record_hash\n",
    "#Search for the record hash using the clowder api and examine the json results\n",
    "    url_start = r'https://clowder.edap-cluster.com/api/search?query='\n",
    "    url_end = r'&order=asc'\n",
    "    api_key = os.environ.get('apiKey')\n",
    "    t.sleep(0.25) #Courtesy sleep time because we're respectful in this house\n",
    "    response = requests.get(url_start+record_hash+url_end, headers = {'X-API-Key': api_key})\n",
    "    json = response.json()\n",
    "#Walk through the json to find the clowder id and file name. Then populate their respective fields in the key\n",
    "    try:\n",
    "        clowder_id = json['results'][0]['id']\n",
    "        file_name = json['results'][0]['name']\n",
    "        key.loc[i,'file_name'] = file_name\n",
    "        key.loc[i,'clowder_id'] = clowder_id\n",
    "        key.loc[i,'found'] = 'found'\n",
    "#Make note of any exceptions (clowder searches with no results).These record hashes are not in clowder.\n",
    "    except:\n",
    "        key.loc[i,'clowder_id'] = ''\n",
    "        key.loc[i,'file_name'] = 'record match but document not found in clowder'\n",
    "        key.loc[i,'found'] = 'found in index files but not found in clowder'\n",
    "    if raw_year != index_year:\n",
    "        key.loc[i,'found'] = 'found but index year and raw year do not match' \n",
    "#Save the key as an excel file to view and make sure everything looks alright.\n",
    "out_path = r\"L:\\PRIV\\ToxValDB\\Document Mapping\\source_efsa\\source_efsa_key_mmille16_09212022.xlsx\"\n",
    "with pd.ExcelWriter(out_path) as writer:\n",
    "    key.to_excel(writer, index = False)\n",
    "print('All done!!')\n",
    "\n",
    "#Count all of the unique clowder ids and how many unique documents were not found\n",
    "\n",
    "#print(list(key['found']).count('not found'))\n",
    "#print(len(np.unique(list((key['clowder_id'])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd77d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL matching probably caught most matches, but lets look for long ref matches just in case any were missed\n",
    "\n",
    "#This array will hold new matches so that I can manually inspect once finished\n",
    "new_matches = []\n",
    "\n",
    "\n",
    "for i in range(len(key)):\n",
    "#Progress check while we look through the key\n",
    "    if i%100 == 0:\n",
    "        print(i)\n",
    "#Only do long ref matching if the document was marked as 'not found' from the url matching\n",
    "    if key.loc[i,'found'] != 'not found':\n",
    "        continue\n",
    "#Then extract the long ref field\n",
    "    long_ref = str(key.loc[i,'long_ref'])\n",
    "    \n",
    "#Now we reuse the search piece of code from before. Only now, we look at the long ref field instead of the url field\n",
    "    for name in index_files:\n",
    "        source = index_files[name]\n",
    "        source = source.fillna('')\n",
    "#Using str.contains as opposed to str.match becuase some of these long_refs have a variety of regex symbols\n",
    "#Can't turn off regex for str.match but can for str.contains\n",
    "        other = source[source['long_ref'].str.contains(long_ref, case = False, regex = False)]\n",
    "#Stop looking once we have found a match\n",
    "        if other.empty == False:\n",
    "            break\n",
    "#If we looked through all the index files and found no match, move on\n",
    "    if other.empty == True:\n",
    "        continue\n",
    "#More rigorous testing. Since we used 'contains', we might not have exact matches.\n",
    "    if long_ref not in new_matches:\n",
    "        index_long_ref = other.loc[other.index[0],'long_ref']\n",
    "        index_citation = other.loc[other.index[0],'citation']\n",
    "        raw_year = str(key.loc[i,'year'])\n",
    "        raw_year = raw_year.replace('.0','')\n",
    "#Now that we know the index long ref CONTAINS the key long ref, see if they are the exact same and made in the same year\n",
    "        if long_ref.lower() == index_long_ref.lower() and raw_year in index_citation:\n",
    "#If all those checks passed, we found a new match and we can repeat the population of the key\n",
    "            new_matches.append(long_ref)\n",
    "#Pull the record hash and search for it in the clowder api\n",
    "            record_hash = other.loc[other.index[0],'record_source_hash']\n",
    "            url_start = r'https://clowder.edap-cluster.com/api/search?query='\n",
    "            url_end = r'&order=asc'\n",
    "            api_key = os.environ.get('apiKey')\n",
    "            t.sleep(0.25) #Courtesy sleep time because we're respectful in this house\n",
    "            response = requests.get(url_start+record_hash+url_end, headers = {'X-API-Key': api_key})\n",
    "            json = response.json()\n",
    "#Look through the retrieved json for the clowder id and file name.\n",
    "            clowder_id = json['results'][0]['id']\n",
    "            file_name = json['results'][0]['name']\n",
    "            key.loc[i,'clowder_id'] = clowder_id\n",
    "            key.loc[i,'file_name'] = file_name\n",
    "            key.loc[i,'record_hash'] = record_hash\n",
    "            key.loc[i,'found'] = 'found'\n",
    "#Check to see how many new matches were found (if any). I do not expect many, so lets see what they are\n",
    "print(f'{len(new_matches)} new matches have been found based on long ref fields')\n",
    "print(new_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ab00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After all of the checks and matches, lets create the final key for efsa that contains all info from efsa raw\n",
    "#as well as the clowder id and file name fields.\n",
    "key = key.fillna('')\n",
    "\n",
    "#Columns from the old key to keep\n",
    "stuff = {'source_hash': key['source_hash'],\n",
    "         'name': key['name'],\n",
    "         'casrn': key['casrn'],\n",
    "         'record_url':key['record_url'], \n",
    "         'long_ref':key['long_ref'],\n",
    "         'year':key['year'],\n",
    "         'found':key['found'],\n",
    "         'clowder_id':key['clowder_id'],\n",
    "         'file_name':key['file_name']\n",
    "        }\n",
    "\n",
    "#Make the final key with the data from the old key\n",
    "final_key = pd.DataFrame(data = stuff)\n",
    "#Save the final key as an excel file so I can double check it.\n",
    "out_path = r\"L:\\PRIV\\ToxValDB\\Document Mapping\\source_efsa\\efsa_final_key_mmille16_09212022.xlsx\"\n",
    "with pd.ExcelWriter(out_path) as writer:\n",
    "    final_key.to_excel(writer, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90204c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we return to the raw file and complete the task\n",
    "\n",
    "for i in range(len(raw)):\n",
    "    #Lets match the url in raw to the url in the key\n",
    "    raw_url = raw.loc[i,'record_url']\n",
    "    row = final_key[final_key['record_url'].str.match(raw_url)]\n",
    "    #Then fill in those clowder id and pdf name fields\n",
    "    raw.loc[i,'clowder_id'] = row.loc[row.index[0],'clowder_id']\n",
    "    raw.loc[i,'pdf_name'] = row.loc[row.index[0],'file_name']\n",
    "\n",
    "#Change all the nan entries to blank strings if there are any.\n",
    "raw = raw.fillna('')\n",
    "\n",
    "#Fill in found/not found based on clowder id entries\n",
    "#Redoing this because the field was populated differently for different errors thrown during the key creation process\n",
    "for i in range(len(raw)):\n",
    "    if raw.loc[i,'clowder_id'] == '':\n",
    "        raw.loc[i,'found'] = 'not found'\n",
    "    else:\n",
    "        raw.loc[i,'found'] = 'found'\n",
    "\n",
    "#Taking stock of what records were and wer not matched\n",
    "arr = raw['found']\n",
    "num1 = list(arr).count('found')\n",
    "num2 = list(arr).count('not found')\n",
    "total = len(raw)\n",
    "\n",
    "#Turn found/not found into percents and print out an update on the status of efsa raw.\n",
    "percent_found = (num1/total)*100\n",
    "percent_missing = (num2/total)*100\n",
    "print(f'{percent_found}% of records in raw have been found and matched to documents in clowder')\n",
    "print(f'{percent_missing}% of records in raw are missing and need to be uploaded to clowder')\n",
    "\n",
    "#Save the updated raw file with matches to an excel file\n",
    "out_path = r\"L:\\PRIV\\ToxValDB\\Document Mapping\\source_efsa\\source_efsa_matched_mmille16_09212022.xlsx\"\n",
    "with pd.ExcelWriter(out_path) as writer:\n",
    "    raw.to_excel(writer, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27c42cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the final key to output an excel spreadsheet with only the uniqe records that were not found\n",
    "\n",
    "#Initialize the dataframe\n",
    "d = {}\n",
    "unique_missing = pd.DataFrame(data = d)\n",
    "j = 0\n",
    "final_key = final_key.fillna('')\n",
    "#Populate it with any key entries with empty clowder id fields\n",
    "for i in range(len(final_key)):\n",
    "    if final_key.loc[i,'clowder_id'] == '':\n",
    "        for column in final_key.columns:\n",
    "            unique_missing.loc[j,column] = final_key.loc[i,column]\n",
    "        j += 1\n",
    "\n",
    "#Save the missing key entries dataframe as an excel sheet\n",
    "out_path = r\"L:\\PRIV\\ToxValDB\\Document Mapping\\source_efsa\\source_efsa_missing_mmille16_09212022.xlsx\"\n",
    "with pd.ExcelWriter(out_path) as writer:\n",
    "    unique_missing.to_excel(writer, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
