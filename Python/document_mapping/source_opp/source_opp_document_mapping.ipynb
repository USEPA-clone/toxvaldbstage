{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "20a2eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modules used\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import urllib.request\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import numpy as np\n",
    "import time as t\n",
    "import os.path\n",
    "import comtypes.client\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3b69a38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#set download directory path\n",
    "p = r\"L:\\PRIV\\ToxValDB\\Document Webscrape\\source_opp2\"\n",
    "#Saves webpage urls and chemical names as key value pairs in links dictionary\n",
    "links = {}\n",
    "\n",
    "#Selenium chrome driver settings\n",
    "s=Service(r'C:\\Users\\mmille16\\Documents\\chromedriver.exe')\n",
    "Options = webdriver.ChromeOptions()\n",
    "Options.headless = True\n",
    "driver = webdriver.Chrome(service=s, options = Options)\n",
    "\n",
    "#Open the linked website and access the data in the table\n",
    "URL = 'https://www.epa.gov/sdwa/2021-human-health-benchmarks-pesticides'\n",
    "driver.get(URL)\n",
    "a = driver.find_elements(By.XPATH,\"//table/tbody[@id = 'data']/tr/td/a\")\n",
    "\n",
    "#Save the final pdf links for metadata upload\n",
    "doc_url = {}\n",
    "\n",
    "\n",
    "for ele in a:\n",
    "#Linked urls in the table could link to the pdfs or a different website to download the file\n",
    "    landing = ele.get_attribute('href')\n",
    "#Linked url is the pdf document url. Easy case\n",
    "    if str(landing).lower().find('.pdf') != -1:\n",
    "        chem_name = ele.text\n",
    "        name = chem_name\n",
    "        pdf_link = landing\n",
    "#Url is not the pdf document url. Might just need to do some string manipulation\n",
    "    else:\n",
    "        name = ele.text\n",
    "#Strip out the \"Exit EPA's website\" string hidden in the text and remove any slashes\n",
    "        bad = \"Exit EPA's website\"\n",
    "        name = name[0:-len(bad)]\n",
    "        chem_name = name.replace('/','')\n",
    "#PDF url could be found with the unique identifier at the end of the linked webpage url (EPA-HQ-numbers)\n",
    "        landing = ele.get_attribute('href').split('/')[-1]\n",
    "        pdf_name = landing[landing.find('EPA'):]\n",
    "        pdf_link = f'https://downloads.regulations.gov/{pdf_name}/content.pdf'\n",
    "    #links[chem_name] = ele.get_attribute('href')\n",
    "    if os.path.isfile(f'{p}/{chem_name}_source_opp.pdf'): #Don't redo work we have already done\n",
    "        continue\n",
    "    try:\n",
    "#Try to save the pdf url as a pdf in the file folder\n",
    "        response = urllib.request.urlopen(pdf_link)\n",
    "        file = open(f'{p}/{chem_name}_source_opp.pdf', 'wb')\n",
    "        file.write(response.read())\n",
    "        file.close()\n",
    "        doc_url[name] = pdf_link\n",
    "    except:\n",
    "#Problematic chemicals. Will need to use Selenium and inspecting to see what is up with these.\n",
    "        links[chem_name] = ele.get_attribute('href')\n",
    "        continue\n",
    "        #print(f'no short cut way to download origin document for {chem_name}')\n",
    "    t.sleep(0.5)\n",
    "driver.close()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4b45f541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not find a document link for Furilazole\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Straggler catching. Not headless so I can see what may be \"wrong\" with some of these\n",
    "#Save path\n",
    "p = r\"L:\\PRIV\\ToxValDB\\Document Webscrape\\source_opp2\"\n",
    "\n",
    "#Selenium chrome driver settings\n",
    "s=Service(r'C:\\Users\\mmille16\\Documents\\chromedriver.exe')\n",
    "Options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=s, options = Options)\n",
    "\n",
    "#Will hold url and chem name for final missing chemcicall opp origin documents\n",
    "missing = {}\n",
    "\n",
    "for k,a in links.items():\n",
    "#Open the first landing page for the chemical linked in the original table\n",
    "    driver.get(a)\n",
    "#This website takes forever to load so long sleep time is required\n",
    "    t.sleep(20)\n",
    "    try:\n",
    "#Look for the download button here\n",
    "        btn = driver.find_element(By.LINK_TEXT,'Download Icon Download')\n",
    "        #print(f'regular regulations landing page contains pdf for {k}')\n",
    "    except:\n",
    "        try:\n",
    "#or look for the download button here\n",
    "            btn = driver.find_element(By.LINK_TEXT,'PDF')\n",
    "            #print(f'different landing page contains pdf for {k}')\n",
    "        except:\n",
    "#Simply cannot find the a download link\n",
    "            print(f'could not find a document link for {k}')\n",
    "            missing[k] = a\n",
    "            continue\n",
    "#Prevent doing work I've already done\n",
    "    if os.path.isfile(f'{p}/{k}_source_opp{btn.get_attribute(\"href\")[-4:]}'):\n",
    "        continue\n",
    "#Download the file with a requests.get. \n",
    "    response = requests.get(btn.get_attribute('href'))\n",
    "#Some of these files cannot be downloaded as pdfs first so use the original extension (.doc or .pdf)\n",
    "    file = open(f'{p}/{k}_source_opp{btn.get_attribute(\"href\")[-4:]}', 'wb')\n",
    "    file.write(response.content)\n",
    "    file.close()\n",
    "    doc_url[k] = btn.get_attribute(\"href\")\n",
    "    #print(f'source document downloaded for {k}')\n",
    "driver.close()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "21c949fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some origin docs had to be downloaded as .doc files instead of .pdf\n",
    "\n",
    "directory1 = r\"L:\\PRIV\\ToxValDB\\Document Webscrape\\source_opp2\"\n",
    "wdFormatPDF = 17\n",
    "\n",
    "#Look for .doc files in the directory\n",
    "for file in os.listdir(directory1):\n",
    "    if str(file).lower().find('.doc') != -1:\n",
    "#Current file path/name and future file path/name\n",
    "        in_file = os.path.join(directory1,file)\n",
    "        out_file = os.path.join(directory1,file.replace('.doc','.pdf'))\n",
    "#Save each word doc as a pdf instead using comtypes.client\n",
    "        word = comtypes.client.CreateObject('Word.Application')\n",
    "        word.Visible = False\n",
    "        t.sleep(3)\n",
    "        doc=word.Documents.Open(in_file)\n",
    "        doc.SaveAs(out_file, FileFormat=wdFormatPDF)\n",
    "        doc.Close()\n",
    "#Remove the old .doc file. No longer necessary since only pdfs are requested.\n",
    "        os.remove(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "18b63a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting metadata for chemicals that have been downloaded\n",
    "url = \"https://www.epa.gov/sdwa/2021-human-health-benchmarks-pesticides\"\n",
    "#Parse html with beautiful soup\n",
    "soup = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "#Go through and remove any attrbutes that are super scripts\n",
    "for sup in soup.select('sup'):\n",
    "    sup.extract()\n",
    "#Final cleaned up table\n",
    "table = pd.read_html(str(soup))[0]\n",
    "\n",
    "#Creating the \"info\" dataframe containing chemicals and urls for documents that have been successfully downloaded\n",
    "d = {'Chemical':doc_url.keys(),\n",
    "    'URL':doc_url.values()}\n",
    "info = pd.DataFrame(data = d)\n",
    "\n",
    "#Metadata merges the cas numbers from the webpage table with the chemicals that have successfully downloaded origin docs\n",
    "metadata = info.merge(table[['CAS Number',table.columns[0]]],\n",
    "                     left_on = 'Chemical',\n",
    "                     right_on = table.columns[0],\n",
    "                     how = 'left').drop(table.columns[0],axis = 1)\n",
    "metadata['Source'] = ['OPP']*len(metadata)\n",
    "#This column will be necessary for uploading the metadata. Will need to merge with clowder log based on file name\n",
    "metadata['file_name'] = [f'{str(x).replace(\"/\",\"\")}_source_opp.pdf' for x in metadata['Chemical']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1a4a1e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the metadata datatframe as an excel spreadsheet\n",
    "out_path = r\"C:\\Users\\mmille16\\OneDrive - Environmental Protection Agency (EPA)\\Profile\\Desktop\\source_opp_metadata.xlsx\"\n",
    "with pd.ExcelWriter(out_path) as writer:\n",
    "    metadata.to_excel(writer, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce29c2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
