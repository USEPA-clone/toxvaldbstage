{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f1c051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and install the necessary packages\n",
    "import requests\n",
    "import pandas as p\n",
    "import numpy as np\n",
    "import time as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f53ae2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = p.read_excel(r'L:\\PRIV\\ToxValDB\\Document Mapping\\1- In Progress\\source_hawc\\hawc_original_12_06_21.xlsx',sheet_name = None)\n",
    "ccte_map = p.read_excel(r'L:\\PRIV\\ToxValDB\\Document Mapping\\toxval_document_map_ccte.xlsx')\n",
    "icf_map = p.read_excel(r'L:\\PRIV\\ToxValDB\\Document Mapping\\toxval_document_map_icf.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f75010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ced462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do document matching on each sheet\n",
    "for i in range(len(df_dict)):\n",
    "    df = df_dict.get('sheet'+str(i)) #load in the i'th sheet in big hawc\n",
    "    for j in range (len(df)):\n",
    "    #replace clowder_id and doc name with empty space. This gives us something to look for when finding docs that need matching\n",
    "        \n",
    "        df.loc[j,'clowder_id'] = '' \n",
    "        df.loc[j,'document_name'] = ''\n",
    "\n",
    "#these arrays will hold the long refs for documents we have matched. Now instead of looping through doc map for\n",
    "#records with repeat titles, we just look back at these arrays. Drastically improves runtime\n",
    "      \n",
    "        used_ref = []\n",
    "        used_title = []\n",
    "        used_clowder_id =[]\n",
    "        used_doc_name = []\n",
    "        \n",
    "\n",
    "#Look for long refs and match them to long refs in icf map. Then replace clowder ids and document names with those in clowder\n",
    "    for j in range (len(df)):\n",
    "        record = str(df.loc[j,'animal_group.experiment.study.full_citation']).lower()\n",
    "        if record in used_ref:\n",
    "            idx = used_ref.index(record)\n",
    "            df.loc[j,'clowder_id'] = used_clowder_id[idx]\n",
    "            df.loc[j,'document_name'] = used_doc_name[idx]\n",
    "        if df.loc[j,'clowder_id'] == '':\n",
    "            for k in range (len(icf_map)):\n",
    "                doc = str(icf_map.loc[k, 'long_ref']).lower()\n",
    "                if record in doc:\n",
    "                    used_ref.append(record)\n",
    "                    used_title.append(df.loc[j,'animal_group.experiment.study.title'])\n",
    "                    used_clowder_id.append(icf_map.loc[k,'clowder_id'])\n",
    "                    used_doc_name.append(icf_map.loc[k,'document_name'])\n",
    "                    df.loc[j,'clowder_id'] = icf_map.loc[k,'clowder_id']\n",
    "                    df.loc[j,'document_name'] = icf_map.loc[k,'document_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97847961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maybe the long refs vary slightly or there was trouble reading accent marks on certain names.\n",
    "#Lets use the \"in\" operator to look for titles in the long refs instead. I found that this picks up a few straggler docs\n",
    "#in each sheet (which amounts to hundreds of records. Pretty good thing for programs instead of by hand inspection)\n",
    "\n",
    "#Again, we load in each sheet individually and perform the same process as above, this time we look at title in long ref\n",
    "#instead of long ref in long ref\n",
    "\n",
    "for i in range(len(df_dict)):\n",
    "    df = df_dict.get('sheet'+str(i))\n",
    "    for j in range (len(df)):\n",
    "        if df.loc[j,'clowder_id'] == '':\n",
    "            record = str(df.loc[j,'animal_group.experiment.study.title']).lower()\n",
    "            if record in used_title:\n",
    "                idx = used_title.index(record)\n",
    "                df.loc[j,'clowder_id'] = used_clowder_id[idx]\n",
    "                df.loc[j,'document_name'] = used_doc_name[idx]\n",
    "        for k in range (len(icf_map)):\n",
    "            doc = str(icf_map.loc[k, 'long_ref']).lower()\n",
    "            if record in doc:\n",
    "                used_title.append(record)\n",
    "                used_clowder_id.append(icf_map.loc[k,'clowder_id'])\n",
    "                used_doc_name.append(icf_map.loc[k,'document_name'])\n",
    "                df.loc[i,'clowder_id'] = icf_map.loc[k,'clowder_id']\n",
    "                df.loc[i,'document_name'] = icf_map.loc[k,'document_name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23a3efe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3590\n",
      "5399\n",
      "66.49379514724949 percent of the records have been matched programatically\n"
     ]
    }
   ],
   "source": [
    "length = 0\n",
    "counter = 0\n",
    "for i in range (len(df_dict)):\n",
    "    df = df_dict.get('sheet'+str(i))\n",
    "    length = length + len(df)\n",
    "    for j in range(len(df)):\n",
    "        if df.loc[j,'clowder_id'] !='':\n",
    "            counter = counter + 1\n",
    "print(counter)\n",
    "print(length)\n",
    "print((counter/length)*100, 'percent of the records have been matched programatically')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f59ca7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check validity of clowder ids\n",
    "\n",
    "api_key = '689d94c2-1a11-4940-b2eb-a0faff85c6be'\n",
    "used_id = []\n",
    "validity = []\n",
    "for i in range (len(df_dict)):\n",
    "    df = df_dict.get('sheet'+str(i))\n",
    "    for j in range (len(df)):\n",
    "        if df.loc[j,'clowder_id'] != '':\n",
    "#Many recoreds come from the same document. Save time by first seeing if we already checked the validity of the clowder id\n",
    "            clowder_id = df.loc[j,'clowder_id']\n",
    "            if clowder_id not in used_id:\n",
    "                t.sleep(1) #Courtesy sleep time\n",
    "#Use requests to interact with clowder api. Save the response code and clowder id in arrays.\n",
    "                response = requests.get('https://clowder.edap-cluster.com/api/files/'+clowder_id, headers = {'X-API-Key': api_key})\n",
    "                used_id.append(clowder_id)\n",
    "                validity.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df44e7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n"
     ]
    }
   ],
   "source": [
    "print(len(validity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7c2ad584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>]\n"
     ]
    }
   ],
   "source": [
    "print(validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9e52519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the validity of each clowder id aquired to make sure it is attached to a clowder document\n",
    "for i in range (len(df_dict)):\n",
    "    df = df_dict.get('sheet'+str(i))\n",
    "    #Create the dataframe column since it wasn't there before\n",
    "    val_arr = np.zeros(len(df))\n",
    "    df['clowder_validity'] = val_arr\n",
    "    for j in range(len(df)):\n",
    "        if df.loc[j,'clowder_id'] in used_id:\n",
    "#Consult the arrays we made before out of clowder ids and their corresponding response codes\n",
    "            idx = used_id.index(df.loc[j,'clowder_id'])\n",
    "            val = validity[idx]\n",
    "#True if clowder id gives a 200 response code, false if it doesnt't\n",
    "            if val.status_code == 200:\n",
    "                df.loc[j,'clowder_validity'] = 'TRUE'\n",
    "            else:\n",
    "                df.loc[j,'clowder_validity'] = 'FALSE'\n",
    "#Note: Not all records were matched to documents and therefore do not have clowder ids. These will remain as 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc876a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3590\n"
     ]
    }
   ],
   "source": [
    "#Count the number of matched records\n",
    "counter2 = 0\n",
    "for i in range(len(df_dict)):\n",
    "    df = df_dict.get('sheet'+str(i))\n",
    "    for j in range(len(df)):\n",
    "        if df.loc[j,'clowder_validity'] != 0:\n",
    "            counter2 = counter2 + 1\n",
    "print(counter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a452b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataframe to an excel spreadsheet\n",
    "writer = p.ExcelWriter('big_hawc_match.xlsx', engine='xlsxwriter')\n",
    "for i in range (len(df_dict)):\n",
    "    df = df_dict.get('sheet'+str(i))\n",
    "    df.to_excel(writer, sheet_name = 'sheet'+str(i), index = False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be42cfc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
